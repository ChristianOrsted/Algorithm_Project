import os
import warnings
warnings.filterwarnings('ignore', category=FutureWarning)
os.environ["SC2PATH"] = "D:/Program Files (x86)/StarCraft II"

from weak_tie_env import WeakTieStarCraft2Env
from mappo_agent import WeakTieMAPPOAgent
from weak_tie_module import WeakTieGraph
import torch
import numpy as np

def evaluate_model(model_path, n_episodes=20):
    """è¯„ä¼°æŒ‡å®šæ¨¡å‹"""
    try:
        # åˆ›å»ºç¯å¢ƒ
        env = WeakTieStarCraft2Env(map_name="1c3s5z")
        env_info = env.get_env_info()
        
        # è·å–ç»´åº¦ä¿¡æ¯
        n_agents = env_info["n_agents"]
        obs_dim = env_info["obs_shape"]
        n_actions = env_info["n_actions"]
        
        # âœ… ä¿®æ­£ï¼šhidden_dim æ”¹ä¸º 256
        agent = WeakTieMAPPOAgent(
            n_agents=n_agents,
            obs_dim=obs_dim,
            act_dim=n_actions,
            hidden_dim=256,  # ğŸ”§ å’Œè®­ç»ƒæ—¶ä¿æŒä¸€è‡´
            lr=0.0003,
            gamma=0.99,
            gae_lambda=0.95,
            clip_param=0.2,
            ppo_epoch=10,
            mini_batch_size=8
        )
        
        # åŠ è½½æ¨¡å‹
        agent.load_model(model_path)
        
        # âœ… ä¿®æ­£ï¼šå‚æ•°åæ”¹ä¸º obs_range
        weak_tie_graph = WeakTieGraph(
            n_agents=n_agents, 
            obs_range=15.0,      # ğŸ”§ ä¿®æ”¹å‚æ•°å
            alpha_quantile=0.3   # ğŸ”§ æ·»åŠ å¿…éœ€å‚æ•°
        )
        
        # è¯„ä¼°æŒ‡æ ‡
        wins = 0
        total_reward = 0
        
        for ep in range(n_episodes):
            env.reset()
            episode_reward = 0
            terminated = False
            
            # åˆå§‹åŒ–éšè—çŠ¶æ€
            actor_hidden = agent.init_hidden(batch_size=1)
            
            step_count = 0
            while not terminated:
                # è·å–ç¯å¢ƒä¿¡æ¯
                obs = env.get_obs()
                avail_actions = env.get_avail_actions()
                positions = env.get_all_unit_positions()
                
                # å­˜æ´»æ©ç 
                alive_mask = np.array([1 if env.agents[i].health > 0 else 0 
                       for i in range(n_agents)])
                
                # è®¡ç®—å¼±è”ç³»å›¾ä¿¡æ¯
                mask_beta, key_agent_idx = weak_tie_graph.compute_graph_info(
                    positions, alive_mask
                )
                
                # é€‰æ‹©åŠ¨ä½œ
                actions, probs, actor_hidden = agent.select_action(
                    obs=obs,
                    avail_actions=avail_actions,
                    mask=mask_beta,
                    key_idx=key_agent_idx,
                    actor_hidden=actor_hidden,
                    deterministic=True
                )
                
                # æ‰§è¡ŒåŠ¨ä½œ
                reward, terminated, info = env.step(actions)
                episode_reward += reward
                step_count += 1
                
                # é˜²æ­¢æ— é™å¾ªç¯
                if step_count > 200:
                    break
            
            # ç»Ÿè®¡ç»“æœ
            if env.win_counted:
                wins += 1
            total_reward += episode_reward
            
            print(f"Episode {ep+1}/{n_episodes}: "
                  f"{'Win' if env.win_counted else 'Loss'} | "
                  f"Reward: {episode_reward:.2f}")
        
        env.close()
        
        win_rate = wins / n_episodes
        avg_reward = total_reward / n_episodes
        
        return win_rate, avg_reward
    
    except Exception as e:
        print(f"âŒ è¯„ä¼°è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        return 0.0, 0.0  # ğŸ”§ ç¡®ä¿å§‹ç»ˆè¿”å›ä¸¤ä¸ªå€¼


# ==================== ä¸»ç¨‹åº ====================
if __name__ == "__main__":
    print("=" * 60)
    print("ğŸ® å¼€å§‹è¯„ä¼°æ¨¡å‹æ€§èƒ½...")
    print("=" * 60)
    
    # è¯„ä¼° Best Model (Ep 1500)
    print("\nğŸ“Š è¯„ä¼° Best Model (Ep 1500)...")
    print("-" * 60)
    best_win_rate, best_avg_reward = evaluate_model("best_model.pt", n_episodes=20)
    print(f"\nâœ… Best Model ç»“æœ:")
    print(f"   èƒœç‡: {best_win_rate*100:.1f}% ({int(best_win_rate*20)}/20)")
    print(f"   å¹³å‡å¾—åˆ†: {best_avg_reward:.2f}")
    
    # è¯„ä¼° Latest Checkpoint (Ep 5000)
    print("\n" + "=" * 60)
    print("ğŸ“Š è¯„ä¼° Latest Checkpoint (Ep 5000)...")
    print("-" * 60)
    latest_win_rate, latest_avg_reward = evaluate_model(
        "checkpoints/ckpt_latest.pt", n_episodes=20
    )
    print(f"\nâœ… Latest Checkpoint ç»“æœ:")
    print(f"   èƒœç‡: {latest_win_rate*100:.1f}% ({int(latest_win_rate*20)}/20)")
    print(f"   å¹³å‡å¾—åˆ†: {latest_avg_reward:.2f}")
    
    # å¯¹æ¯”ç»“æœ
    print("\n" + "=" * 60)
    print("ğŸ“ˆ å¯¹æ¯”ç»“æœ:")
    print("-" * 60)
    
    if best_win_rate > latest_win_rate:
        print(f"ğŸ† Best Model (Ep 1500) èƒœç‡æ›´é«˜")
        print(f"   ä¼˜åŠ¿: {(best_win_rate - latest_win_rate)*100:.1f}%")
        print(f"\nğŸ’¡ å»ºè®®: åœ¨ train_smac.py ä¸­è®¾ç½® RESUME_SOURCE='best'")
    elif latest_win_rate > best_win_rate:
        print(f"ğŸ† Latest Checkpoint (Ep 5000) èƒœç‡æ›´é«˜")
        print(f"   ä¼˜åŠ¿: {(latest_win_rate - best_win_rate)*100:.1f}%")
        print(f"\nğŸ’¡ å»ºè®®: ç»§ç»­ä½¿ç”¨ RESUME_SOURCE='latest'")
    else:
        print(f"âš–ï¸ ä¸¤ä¸ªæ¨¡å‹èƒœç‡ç›¸åŒ")
        print(f"\nğŸ’¡ å»ºè®®: è€ƒè™‘é‡æ–°è®­ç»ƒï¼ˆå¯èƒ½é™·å…¥å±€éƒ¨æœ€ä¼˜ï¼‰")
    
    if best_avg_reward > latest_avg_reward:
        print(f"\nğŸ“Š å¹³å‡å¾—åˆ†: Best Model æ›´é«˜ ({best_avg_reward:.2f} vs {latest_avg_reward:.2f})")
    elif latest_avg_reward > best_avg_reward:
        print(f"\nğŸ“Š å¹³å‡å¾—åˆ†: Latest Checkpoint æ›´é«˜ ({latest_avg_reward:.2f} vs {best_avg_reward:.2f})")
    
    print("\n" + "=" * 60)