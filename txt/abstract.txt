cemtealized training: 集中式训练
训练时，所有智能体的信息（观测、动作、奖励）都汇总到一个中央控制器进行学习。
特点
✅ 全局视野：能看到所有智能体的状态
✅ 协调性好：容易学习团队协作策略
❌ 可扩展性差：智能体数量增加时，计算复杂度爆炸
❌ 单点故障：中央控制器崩溃则全体瘫痪

decentralized excution training: 去中心化执行
执行时，每个智能体独立决策，只根据自己的局部观测选择动作，不依赖中央控制器。
特点
✅ 鲁棒性强：单个智能体故障不影响整体
✅ 可扩展：智能体数量增加不影响执行效率
✅ 现实可行：符合真实世界的分布式系统（如无人机集群、自动驾驶车队）
❌ 协调困难：每个智能体只看到局部信息，难以全局协作

MAPPO是Multi-Agent Proximal Policy Optimization（多智能体近端策略优化）的缩写

Hindsight Experience Replay (HER)
在传统强化学习中,如果智能体没有达到目标,就得不到任何奖励,导致学习效率极低。HER 的核心创新是:即使失败了,也能从中学习。
具体做法是:
智能体尝试达到目标 A,但实际到达了位置 B（失败）
HER 会"事后诸葛亮"地假设:"如果我的目标本来就是 B 呢?"
这样这次经历就变成了一次成功的经验,可以用来学习

Neighborhood Cognitive Consistency (邻域认知一致性)
这个概念基于认知一致性理论的扩展,强调个体倾向于保持其"邻域"（neighborhood）内的认知、态度和信念的一致性。
"邻域"可以指:
社会网络中的邻近节点（朋友、同事、社区成员）
认知结构中相关的概念和信念
地理或社交空间中的邻近个体
基本原理
人们倾向于:
与周围环境保持一致: 调整自己的观点以匹配社交圈的主流看法
减少认知失调: 当发现邻域内存在矛盾信念时,会感到不适并寻求解决
选择性接触: 主动寻找与自己观点一致的社交邻域

CTDE: Centralized Training with Decentralized Execution (集中式训练与分散式执行)
核心思想
这是多智能体强化学习中的一种重要训练范式，结合了两种模式的优点：
集中式训练 (Centralized Training)
训练阶段：可以访问全局信息
包括：所有智能体的观测、动作、奖励，甚至环境的真实状态
优势：利用全局信息更好地评估每个智能体的行为价值
分散式执行 (Decentralized Execution)
执行阶段：每个智能体独立决策
只依赖：自己的局部观测历史
优势：符合实际应用场景（智能体之间可能无法实时通信）
