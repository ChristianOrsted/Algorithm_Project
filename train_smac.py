from weak_tie_env import WeakTieStarCraft2Env
import numpy as np
import time
import os
import sys
import glob
from datetime import datetime
from weak_tie_module import WeakTieGraph
from mappo_agent import WeakTieMAPPOAgent
import torch

# ==============================================================================
# ğŸ”¥ ä¼˜åŒ–åçš„è®­ç»ƒé…ç½® - ä¿®å¤"é€æ­»è·¯çº¿"é—®é¢˜
# ==============================================================================
MAP_NAME = "1c3s5z"
N_EPISODES = 15000

# ã€ä¿®æ”¹1ã€‘è°ƒæ•´ Batch å’Œ PPO å‚æ•°ï¼Œæå‡è®­ç»ƒç¨³å®šæ€§
BATCH_SIZE = 32
MINI_BATCH_SIZE = 16      # ä» 32 é™ä½åˆ° 16ï¼Œå¢åŠ æ›´æ–°é¢‘ç‡
PPO_EPOCH = 15            # ä» 10 æå‡åˆ° 15ï¼Œå……åˆ†åˆ©ç”¨ç»éªŒ

OBS_RANGE = 15.0
EVAL_INTERVAL = 500       # è¯„ä¼°é—´éš”
EVAL_EPISODES = 50        # ã€ä¿®æ”¹2ã€‘ä» 20 æå‡åˆ° 50ï¼Œå‡å°‘è¿æ°”å› ç´ 
MODEL_PATH = "best_model.pt"

# --- æ–­ç‚¹ç»­è®­é…ç½® ---
CHECKPOINT_DIR = "checkpoints"
CHECKPOINT_INTERVAL = 1000

# ã€ä¿®æ”¹3ã€‘æ¢å¤ç­–ç•¥æ”¹ä¸º "none"ï¼Œæ¸…é™¤é”™è¯¯ç»éªŒé‡æ–°è®­ç»ƒ
# å¯é€‰å€¼:
#   "ckpt"  -> å¼ºåˆ¶åŠ è½½ ckpt_latest.pt
#   "best"  -> å¼ºåˆ¶åŠ è½½ best_model.pt
#   "latest"-> è‡ªåŠ¨æ¯”è¾ƒä¸¤è€…ï¼Œè°çš„è½®æ•°å¤§åŠ è½½è°
#   "none"  -> å¼ºåˆ¶ä»å¤´å¼€å§‹ï¼ˆæ¨èç”¨äºä¿®å¤é”™è¯¯ç­–ç•¥ï¼‰
RESUME_SOURCE = "none"

# æé€Ÿä¼˜åŒ–
GRAPH_UPDATE_INTERVAL = 3
STEP_DELAY = 0.0

# ã€ä¿®æ”¹4ã€‘ç†µç³»æ•° - æ¢å¤æ¢ç´¢èƒ½åŠ›ï¼ˆæ ¸å¿ƒä¿®å¤ï¼‰
ENTROPY_START = 0.05      # ä» 0.01 æå‡åˆ° 0.05
ENTROPY_END = 0.001       # ä» 0.01 é™ä½åˆ° 0.001
ENTROPY_DECAY_EPISODES = 3000  # ä» 1 å»¶é•¿åˆ° 3000ï¼Œè®©æ¢ç´¢è´¯ç©¿å‰åŠè®­ç»ƒ

# ã€ä¿®æ”¹5ã€‘å­¦ä¹ ç‡é™ä½ï¼Œé˜²æ­¢é—å¿˜è¿‡å¿«
if MAP_NAME in ["1c3s5z", "50m", "10m_vs_11m"]:
    HIDDEN_DIM = 256
    LR = 0.0001           # ä» 0.0003 æˆ– 0.0005 é™ä½
else:
    HIDDEN_DIM = 128
    LR = 0.0001


# ==============================================================================
# æ—¥å¿—ç³»ç»Ÿ
# ==============================================================================
class Logger:
    def __init__(self, log_file):
        self.terminal = sys.stdout
        self.log = open(log_file, 'a', encoding='utf-8')

    def write(self, message):
        self.terminal.write(message)
        self.log.write(message)
        self.log.flush()

    def flush(self):
        self.terminal.flush()
        self.log.flush()


def setup_logger(log_dir='log'):
    if not os.path.exists(log_dir):
        os.makedirs(log_dir)
    log_file = os.path.join(log_dir, f'training_log_{MAP_NAME}.txt')
    logger = Logger(log_file)
    sys.stdout = logger
    sys.stderr = logger
    print(f"\n{'=' * 60}")
    print(f"å¯åŠ¨æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"{'=' * 60}\n")
    return log_file


def get_current_entropy(episode):
    """æ¸è¿›å¼ç†µè¡°å‡"""
    if episode > ENTROPY_DECAY_EPISODES:
        return ENTROPY_END
    frac = 1.0 - (episode / ENTROPY_DECAY_EPISODES)
    return max(ENTROPY_END, ENTROPY_END + frac * (ENTROPY_START - ENTROPY_END))


def peek_model_episode(path, device):
    """åªè¯»å–æ¨¡å‹æ–‡ä»¶ä¸­çš„ episode ä¿¡æ¯ï¼Œä¸åŠ è½½å‚æ•°"""
    if not os.path.exists(path):
        return None
    try:
        ckpt = torch.load(path, map_location=device)
        return ckpt.get('episode', 0)
    except Exception as e:
        print(f"è­¦å‘Š: æ— æ³•è¯»å–æ–‡ä»¶ {path}: {e}")
        return None


def run_episode(env, agent, wt_graph, train_mode=True, episode_num=0):
    """
    ã€ä¿®æ”¹6ã€‘å¥–åŠ±å¡‘å½¢ä¼˜åŒ– - å¢å¼ºä¿¡å·å¼ºåº¦
    """
    obs, state = env.reset()
    terminated = False
    episode_reward = 0
    raw_episode_reward = 0
    actor_hidden = agent.init_hidden(batch_size=1)

    episode_buffer = {'obs': [], 'acts': [], 'rewards': [], 'dones': [],
                      'avails': [], 'probs': [], 'masks': [], 'keys': []}

    step_count = 0
    last_mask_beta = None
    last_key_agent_idx = None

    while not terminated:
        avail_actions = env.get_avail_actions()
        positions = env.get_all_unit_positions()
        alive_mask = np.any(positions != 0, axis=1)

        if step_count % GRAPH_UPDATE_INTERVAL == 0:
            mask_beta, key_agent_idx = wt_graph.compute_graph_info(positions, alive_mask)
            last_mask_beta = mask_beta
            last_key_agent_idx = key_agent_idx
        else:
            mask_beta = last_mask_beta
            key_agent_idx = last_key_agent_idx

        step_count += 1

        actions, probs, next_hidden = agent.select_action(
            obs, avail_actions, mask_beta, key_agent_idx, actor_hidden,
            deterministic=(not train_mode)
        )

        reward, terminated, info = env.step(actions)
        next_obs = env.get_obs()

        # ã€ä¿®æ”¹6ã€‘æ”¹è¿›å¥–åŠ±å¡‘å½¢
        # åŸä»£ç : shaped_reward = reward / 5.0  # è¿‡åº¦å‰Šå¼±ä¿¡å·
        # æ–°ç­–ç•¥: æ—©æœŸæ­»äº¡æƒ©ç½š + è½»åº¦ç¼©æ”¾
        if episode_reward == 0 and step_count < 50 and terminated and not info.get('battle_won', False):
            # æ—©æœŸæ­»äº¡ï¼ˆé€æ­»ï¼‰ä¸¥é‡æƒ©ç½š
            shaped_reward = reward - 0.3
        else:
            # æ­£å¸¸æˆ˜æ–—ï¼šä¿æŒåŸå§‹å¥–åŠ±ï¼ˆæˆ–è½»åº¦ç¼©æ”¾ï¼‰
            shaped_reward = reward  # ç›´æ¥ä½¿ç”¨åŸå§‹å¥–åŠ±ï¼Œè®©ä¿¡å·æ›´å¼º
            # å¦‚æœè§‰å¾—æ³¢åŠ¨å¤ªå¤§ï¼Œå¯ä»¥æ”¹ä¸º: shaped_reward = reward / 2.0

        if train_mode:
            episode_buffer['obs'].append([obs])
            episode_buffer['acts'].append([actions])
            episode_buffer['rewards'].append([[shaped_reward] * len(actions)])
            episode_buffer['dones'].append([[float(terminated)] * len(actions)])
            episode_buffer['avails'].append([avail_actions])
            episode_buffer['probs'].append([probs])
            episode_buffer['masks'].append([mask_beta])
            episode_buffer['keys'].append([key_agent_idx])

        obs = next_obs
        actor_hidden = next_hidden
        episode_reward += shaped_reward
        raw_episode_reward += reward

    is_win = info.get('battle_won', False)
    return episode_reward, raw_episode_reward, is_win, episode_buffer, None


def main():
    setup_logger()

    print(f"åœ°å›¾: {MAP_NAME} | ç›®æ ‡å›åˆ: {N_EPISODES}")
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Device: {device}")

    # ===== å…³é—­æ¸²æŸ“ï¼Œå¯ç”¨æ— å¤´æ¨¡å¼ =====
    os.environ["SDL_VIDEODRIVER"] = "dummy"
    
    try:
        env = WeakTieStarCraft2Env(
            map_name=MAP_NAME, 
            difficulty="1",  # æœ€ä½éš¾åº¦ï¼Œä¾¿äºåˆæœŸå­¦ä¹ 
            window_size_x=640,
            window_size_y=480
        )
        print("ç¯å¢ƒå·²å¯åŠ¨ï¼ˆæ— æ¸²æŸ“æ¨¡å¼ï¼Œæ€§èƒ½ä¼˜åŒ–å·²å¼€å¯ï¼‰")
    except Exception as e:
        print(f"ç¯å¢ƒå¯åŠ¨å¤±è´¥: {e}")
        try:
            env = WeakTieStarCraft2Env(
                map_name=MAP_NAME,
                difficulty="1",
                window_size_x=640,
                window_size_y=480,
                disable_fog=False
            )
            print("ç¯å¢ƒå·²å¯åŠ¨ï¼ˆå¤‡é€‰æ— æ¸²æŸ“æ¨¡å¼ï¼‰")
        except Exception as e2:
            print(f"å¤‡é€‰æ–¹æ¡ˆä¹Ÿå¤±è´¥: {e2}")
            return

    env_info = env.get_env_info()
    n_agents = env_info["n_agents"]
    n_actions = env_info["n_actions"]
    obs_dim = env_info["obs_shape"]

    wt_graph = WeakTieGraph(n_agents, obs_range=OBS_RANGE, alpha_quantile=0.3)
    agent = WeakTieMAPPOAgent(n_agents, obs_dim, n_actions,
                              hidden_dim=HIDDEN_DIM, lr=LR,
                              ppo_epoch=PPO_EPOCH, mini_batch_size=MINI_BATCH_SIZE)

    # ==========================================================================
    # æ™ºèƒ½æ¨¡å‹åŠ è½½é€»è¾‘ + è‡ªåŠ¨åˆ›å»ºæ–‡ä»¶å¤¹
    # ==========================================================================
    if not os.path.exists(CHECKPOINT_DIR):
        os.makedirs(CHECKPOINT_DIR)
        print(f"å·²åˆ›å»ºæ–‡ä»¶å¤¹: {CHECKPOINT_DIR}")
    
    model_dir = os.path.dirname(MODEL_PATH)
    if model_dir and not os.path.exists(model_dir):
        os.makedirs(model_dir)
        print(f"å·²åˆ›å»ºæ–‡ä»¶å¤¹: {model_dir}")

    ckpt_path = os.path.join(CHECKPOINT_DIR, "ckpt_latest.pt")
    best_path = MODEL_PATH

    ckpt_ep = peek_model_episode(ckpt_path, device)
    best_ep = peek_model_episode(best_path, device)

    print(f"\nå­˜æ¡£çŠ¶æ€æ‰«æ:")
    print(f"[Ckpt]  è‡ªåŠ¨å­˜æ¡£: {'ä¸å­˜åœ¨' if ckpt_ep is None else f'Ep {ckpt_ep}'}")
    print(f"[Best]  æœ€ä½³æ¨¡å‹: {'ä¸å­˜åœ¨' if best_ep is None else f'Ep {best_ep}'}")

    start_episode = 0
    target_file = None

    if RESUME_SOURCE == "ckpt":
        if ckpt_ep is not None:
            target_file = ckpt_path
            print(f"ç­–ç•¥: å¼ºåˆ¶åŠ è½½ Ckpt")
        else:
            print(f"ç­–ç•¥è¦æ±‚åŠ è½½ Ckpt ä½†æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°†å°è¯• Best æˆ–é‡å¤´å¼€å§‹ã€‚")
            if best_ep is not None: target_file = best_path

    elif RESUME_SOURCE == "best":
        if best_ep is not None:
            target_file = best_path
            print(f"ç­–ç•¥: å¼ºåˆ¶åŠ è½½ Best Model")
        else:
            print(f"ç­–ç•¥è¦æ±‚åŠ è½½ Best ä½†æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°†å°è¯• Ckptã€‚")
            if ckpt_ep is not None: target_file = ckpt_path

    elif RESUME_SOURCE == "latest":
        print(f"ç­–ç•¥: è‡ªåŠ¨é€‰æ‹©è½®æ•°æœ€æ–°çš„æ¨¡å‹")
        ep_c = ckpt_ep if ckpt_ep is not None else -1
        ep_b = best_ep if best_ep is not None else -1

        if ep_c > ep_b:
            target_file = ckpt_path
        elif ep_b > -1:
            target_file = best_path

    if target_file:
        print(f"æœ€ç»ˆå†³å®šåŠ è½½: {target_file}")
        start_episode = agent.load_model(target_file)
    else:
        print(f"æœªæ‰¾åˆ°å¯ç”¨æ¨¡å‹æˆ–ç­–ç•¥è®¾ä¸º noneï¼Œä»å¤´å¼€å§‹è®­ç»ƒã€‚")

    if start_episode >= N_EPISODES:
        print("è®­ç»ƒç›®æ ‡å·²è¾¾æˆï¼Œæ— éœ€ç»§ç»­è®­ç»ƒã€‚")
        env.close()
        return

    # ==========================================================================

    # ã€ä¿®æ”¹7ã€‘å¤šæŒ‡æ ‡è¯„ä¼°ï¼šåŒæ—¶è·Ÿè¸ªèƒœç‡å’Œå¹³å‡å¾—åˆ†
    best_win_rate = 0.0
    best_avg_reward = -999.0  # æ–°å¢ï¼šé˜²æ­¢ä½è´¨é‡æ¨¡å‹è¢«ä¿å­˜
    total_wins = 0
    recent_raw_rewards = []
    batch_buffer = []

    training_start_time = time.time()

    print(f"\næ­£å¼å¼€å§‹è®­ç»ƒ (ä» Ep {start_episode + 1} åˆ° {N_EPISODES})")
    print(f"é…ç½®: LR={LR}, Entropy={ENTROPY_START}â†’{ENTROPY_END}, PPO_Epoch={PPO_EPOCH}\n")

    for episode in range(start_episode + 1, N_EPISODES + 1):
        curr_entropy = get_current_entropy(episode)

        _, raw_reward, is_win, buffer, _ = run_episode(env, agent, wt_graph, train_mode=True, episode_num=episode)

        batch_buffer.append((buffer, None))
        if is_win: total_wins += 1
        recent_raw_rewards.append(raw_reward)

        if len(batch_buffer) >= BATCH_SIZE:
            loss = agent.update_batch(batch_buffer, entropy_coef=curr_entropy)
            batch_buffer = []
            print(f"Ep {episode} | Loss: {loss:.4f} | Ent: {curr_entropy:.3f}")

        if episode % 10 == 0:
            res_str = "WIN" if is_win else "LOSE"
            elapsed_time = time.time() - training_start_time
            print(
                f"Ep {episode} | RawRew: {raw_reward:.2f} | {res_str} | Wins: {total_wins} | Time: {elapsed_time / 60:.1f}m")

        if episode % 200 == 0:
            avg_rew = np.mean(recent_raw_rewards) if recent_raw_rewards else 0
            current_session_episodes = episode - start_episode
            win_rate = total_wins / current_session_episodes * 100 if current_session_episodes > 0 else 0

            print(f"\n=== [è¶‹åŠ¿æŠ¥å‘Š] Ep {episode} ===")
            print(f"å¹³å‡å¾—åˆ†: {avg_rew:.2f}")
            print(f"å½“å‰è¿è¡Œèƒœåœº: {total_wins}/{current_session_episodes} ({win_rate:.2f}%)")
            print(f"æ¢ç´¢ç³»æ•°: {curr_entropy:.4f}")
            print(f"============================\n")
            recent_raw_rewards = []

        # ã€ä¿®æ”¹8ã€‘æ”¹è¿›è¯„ä¼°å’Œä¿å­˜é€»è¾‘
        if episode % EVAL_INTERVAL == 0:
            print(f"\n>>> è¯„ä¼°æ¨¡å¼ ({EVAL_EPISODES}å±€)...")
            eval_wins = 0
            eval_rewards = []
            for _ in range(EVAL_EPISODES):
                _, raw_rew, win, _, _ = run_episode(env, agent, wt_graph, train_mode=False)
                if win: eval_wins += 1
                eval_rewards.append(raw_rew)

            curr_win_rate = eval_wins / EVAL_EPISODES
            avg_eval_reward = np.mean(eval_rewards)
            print(f">>> è¯„ä¼°èƒœç‡: {curr_win_rate * 100:.1f}% | å¹³å‡å¾—åˆ†: {avg_eval_reward:.2f}")

            # å¤šæŒ‡æ ‡è¯„ä¼°ï¼šä¼˜å…ˆèƒœç‡ï¼Œå…¶æ¬¡å¾—åˆ†
            should_save = False
            if curr_win_rate > best_win_rate:
                should_save = True
                save_reason = f"èƒœç‡æå‡ {best_win_rate:.1%} â†’ {curr_win_rate:.1%}"
            elif curr_win_rate == best_win_rate and curr_win_rate > 0:
                if avg_eval_reward > best_avg_reward:
                    should_save = True
                    save_reason = f"èƒœç‡æŒå¹³ä½†å¾—åˆ†æå‡ {best_avg_reward:.2f} â†’ {avg_eval_reward:.2f}"
                else:
                    print(f">>> èƒœç‡æŒå¹³ä½†å¾—åˆ†æœªæå‡ï¼Œä¿ç•™åŸæ¨¡å‹")
            
            if should_save:
                best_win_rate = curr_win_rate
                best_avg_reward = avg_eval_reward
                agent.save_model(MODEL_PATH, episode)
                print(f">>> æœ€ä½³æ¨¡å‹å·²æ›´æ–° @ Ep {episode}")
                print(f"    åŸå› : {save_reason}")

        if episode % CHECKPOINT_INTERVAL == 0:
            ckpt_save_path = os.path.join(CHECKPOINT_DIR, "ckpt_latest.pt")
            agent.save_model(ckpt_save_path, episode)
            print(f">>> å®‰å…¨å­˜æ¡£å·²æ›´æ–°: {ckpt_save_path}")

    env.close()
    print("\nè®­ç»ƒå®Œæˆï¼")
    print(f"æœ€ç»ˆæœ€ä½³èƒœç‡: {best_win_rate:.1%} (å¾—åˆ† {best_avg_reward:.2f})")


if __name__ == "__main__":
    main()
